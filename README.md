# ğŸš€ Retail Sales Forecasting Project

## ğŸ“Š Overview

This project implements a **production-ready forecasting pipeline** for daily product sales using multiple time series models:

- **ğŸ“ˆ ARIMA** - Traditional statistical forecasting
- **ğŸ”® Prophet** - Facebook's seasonal forecasting model  
- **ğŸ§  LSTM** - Deep learning neural network approach

## âœ¨ Key Features

- **ğŸ¤– Automated Pipeline** - Scheduled retraining and data monitoring
- **ğŸ“Š Interactive Dashboard** - Real-time model performance monitoring with **actual sales amounts**
- **ğŸ“ˆ Sales Data Visualization** - Date and product category-based sales analysis
- **ğŸ”¬ Experiment Tracking** - MLflow integration for model versioning
- **ğŸ“§ Smart Notifications** - Email/Slack alerts for pipeline events
- **ğŸ¯ Model Selection** - Automatic best model identification
- **ğŸ”® Forecast Comparison** - Predicted vs actual sales amounts
- **â˜ï¸ Cloud Ready** - Docker, Kubernetes, and cloud deployment support

## ğŸ—ï¸ Architecture

The system uses **Prefect** for workflow orchestration, **MLflow** for experiment tracking, and **Streamlit** for the monitoring dashboard. It's designed to run automatically with minimal human intervention while providing comprehensive monitoring and alerting capabilities.

## Project Structure
```
forecasting_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ .gitkeep
â”œâ”€â”€ models/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train_arima.py
â”‚   â”œâ”€â”€ train_prophet.py
â”‚   â”œâ”€â”€ train_lstm.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ run_dashboard.py
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â””â”€â”€ start_automated_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚   â””â”€â”€ monitor.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ arima_model.py
â”‚   â”‚   â”œâ”€â”€ prophet_model.py
â”‚   â”‚   â”œâ”€â”€ lstm_model.py
â”‚   â”‚   â”œâ”€â”€ lstm_dataloader.py
â”‚   â”‚   â””â”€â”€ model_selector.py
â”‚   â”œâ”€â”€ notifications/
â”‚   â”‚   â””â”€â”€ alert_manager.py
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ retrain_flow.py
â”‚   â”œâ”€â”€ tracking/
â”‚   â”‚   â””â”€â”€ mlflow_utils.py
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ dashboard.py
â”œâ”€â”€ config.example.ini
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ DEPLOYMENT.md
â””â”€â”€ .gitignore
```

## ğŸš€ Quick Start

### Option 1: Automated Setup (Recommended)

**Linux/Mac:**
```bash
git clone https://github.com/yourusername/forecasting_project.git
cd forecasting_project
chmod +x setup.sh
./setup.sh
```

**Windows:**
```bash
git clone https://github.com/yourusername/forecasting_project.git
cd forecasting_project
setup.bat
```

### Option 2: Manual Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/forecasting_project.git
   cd forecasting_project
   ```

2. **Create virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Linux/Mac
   # OR
   venv\Scripts\activate     # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Setup configuration**
   ```bash
   cp config.example.ini config.ini
   ```

5. **Add your data**
   ```bash
   # Place your CSV file in data/raw/
   cp your_sales_data.csv data/raw/retail_sales_dataset.csv
   ```

6. **Run the pipeline**
   ```bash
   python scripts/preprocess.py
   python scripts/train_prophet.py
   python scripts/train_arima.py
   python scripts/train_lstm.py
   ```

7. **Start the dashboard**
   ```bash
   python scripts/run_dashboard.py
   ```

8. **Open dashboard**
   ```
   http://localhost:8501
   ```

## Setup (Legacy)

## Usage

### ğŸš€ Automated Pipeline (Recommended)

The pipeline is designed to run automatically without manual intervention:

1. **Start the automated pipeline:**
   ```bash
   python scripts/start_automated_pipeline.py
   ```

   This will:
   - âœ… Start Prefect server and agent
   - âœ… Create scheduled retraining deployment (daily at 2 AM UTC)
   - âœ… Start data monitoring (checks for new data every 5 minutes)
   - âœ… Automatically trigger retraining when new data is detected
   - âœ… Send email notifications for pipeline success/failure

2. **Monitor the pipeline:**
   - **Dashboard:** http://localhost:8501
   - **MLflow UI:** http://localhost:5000
   - **Prefect UI:** http://localhost:4200

### Manual Execution
- Training scripts: `scripts/train_arima.py`, `scripts/train_prophet.py`, `scripts/train_lstm.py`
- Inference: `scripts/inference.py`
- Evaluation: `scripts/evaluate.py`
- Preprocessing: `scripts/preprocess.py`
- Dashboard: `python scripts/run_dashboard.py`
- Sales Demo: `python scripts/demo_sales_dashboard.py`

### Pipeline Components
- **Scheduled retraining**: Prefect flow in `src/pipeline/retrain_flow.py` and deployment in `retrain_deployment.yaml`
- **Data monitoring**: `src/data/monitor.py` for detecting new data
- **Pipeline testing**: `scripts/test_pipeline.py` to validate pipeline functionality
- **Notifications**: `src/notifications/alert_manager.py` for email/Slack alerts
- **Model selection**: `src/models/model_selector.py` for automatic best model selection
- **Monitoring dashboard**: `src/dashboard/dashboard.py` for real-time monitoring

## ğŸ“Š Enhanced Dashboard Features

The dashboard now includes comprehensive sales data visualization beyond just MAE metrics:

### ğŸ“ˆ Sales Data Tab
- **Sales Overview**: Total sales, average daily sales, product count, date range
- **Time Series Analysis**: Interactive charts with date range and product filters
- **Product Analysis**: Performance metrics, sales distribution, heatmaps
- **Forecast Comparison**: Predicted vs actual sales amounts (when models are trained)
- **Data Table**: Interactive table with filtering and export capabilities

### ğŸ¯ Key Benefits
1. **ğŸ“Š Actual Business Impact**: See real sales amounts, not just error metrics
2. **ğŸ“… Time-based Analysis**: Track sales trends over specific date ranges
3. **ğŸ·ï¸ Product Performance**: Compare sales across different product categories
4. **ğŸ”® Forecast Comparison**: Compare predicted vs actual sales amounts
5. **ğŸ“Š Interactive Filtering**: Filter by date ranges and specific products
6. **ğŸ“¥ Data Export**: Download filtered data for further analysis

### ğŸš€ Dashboard Usage
```bash
# Start the dashboard
python scripts/run_dashboard.py

# Demo sales features
python scripts/demo_sales_dashboard.py
```

**Dashboard URL:** http://localhost:8501
- **Model Performance Tab**: Traditional MAE/RMSE metrics
- **Sales Data Tab**: Actual sales amounts and business insights
- **System Status Tab**: Pipeline health and notifications

### MLflow Tracking
- Experiment tracking: see `src/tracking/mlflow_utils.py`
- Model registry and versioning
- Performance metrics logging

## â˜ï¸ Cloud Deployment

For cloud deployment instructions, see [DEPLOYMENT.md](DEPLOYMENT.md).

### Quick Cloud Setup

1. **Copy configuration:**
   ```bash
   cp config.example.ini config.ini
   ```

2. **Update with your cloud settings:**
   - Database credentials
   - Cloud storage (S3/GCS/Azure)
   - Email/Slack notifications
   - MLflow/Prefect server URLs

3. **Deploy using your preferred method:**
   - Docker: `docker-compose up -d`
   - Kubernetes: `kubectl apply -f k8s/`
   - Cloud platforms: See DEPLOYMENT.md

## Notebooks
- EDA: `notebooks/eda.ipynb`
- Model comparison: `notebooks/model_comparison.ipynb`

## Configuration
- Edit `config.ini` for model and pipeline parameters.

## Production Pipeline

### Automatic Data Handling
The pipeline automatically:
1. **Detects new data** - Monitors raw data file for changes
2. **Validates data** - Checks format, columns, and data quality
3. **Preprocesses data** - Handles missing values, outliers, feature engineering
4. **Retrains models** - Updates all models with latest data
5. **Runs inference** - Generates new forecasts
6. **Logs everything** - Tracks all steps in MLflow

### Error Handling & Monitoring
- **Retry logic** - Failed tasks retry up to 3 times
- **Timeout protection** - 1-hour timeout per pipeline run
- **Comprehensive logging** - All steps logged with timestamps
- **Data validation** - Ensures data quality before processing

### Deployment
```bash
# Deploy the pipeline
prefect deploy -f retrain_deployment.yaml

# Start the agent
prefect agent start

# Test the pipeline
python scripts/test_pipeline.py
```
